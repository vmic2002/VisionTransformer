{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TnT80MqQqGq"
      },
      "source": [
        "## TD2. Vision Transformers\n",
        "By Nicolas Dufour\n",
        "\n",
        "In this TD, we will implement the Transformers architecture. Transformers has been a key architecture in deep learning for the past 5 years.\n",
        "\n",
        "It has first began with NLP, then came audio and finally, since 2020, computer vision.\n",
        "We will implement every block that makes a transformer from scratch and we will try to create a deep understanding of what is happening.\n",
        "Here is a figure for the transformer architecture:\n",
        "\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Miruna-Gheata/publication/355339249/figure/fig1/AS:1079476452622337@1634378650979/Encoder-decoder-architecture-of-the-Transformer-developed-by-Vaswani-et-al-28.ppm\" width=768>\n",
        "\n",
        "## Instructions\n",
        "In `pytorch` you must avoid using for loops at all cost. It's almost always possible to find a vectorized version of the operation you want to implement.\n",
        "\n",
        "**In this TP, the only for-loop you can do is the training loop.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWexUvqUQqGr",
        "outputId": "faae0d91-43a7-4384-cfba-d4fad469026d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BebXtCPQqGs",
        "outputId": "ee9da1b2-56a7-4569-8be6-72291d31545f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "import math\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import requests\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rqnohGkQqGt"
      },
      "source": [
        "# Part 1. The attention mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MA_xHcwQqGt"
      },
      "source": [
        "### Implementing the scaled dot-product attention mechanism\n",
        "\n",
        "The transformer architecture is built around one key block: The attention.\n",
        "The idea behind attention is the following. Imagine you want to retrieve information from a dictionary. The dictionnary is indexed by keys which maps to a particular value. Now, you have a query which will be matched against the keys of the dict and if you have a match, you will retrieve the associated value.\n",
        "Attention is very similar to this simple retrieval example. Now, with real data, we don't have this structure, we however are going to learn to create it.\n",
        "\n",
        "We have 2 sets of vectors (also named tokens). One is $X_{to}$ which is the destination set. We want to be able to map this set of tokens to queries. We achieve this by doing a linear projection of $X_{to}$ to obatain:  $Q = W_QX_{to}$\n",
        "\n",
        "The other set is $X_{from}$ the set from which we want to retrieve information. We will need to extract both keys and values from this set. We therefore do 2 linear projections of $X_{from}$ to obtain:  $K = W_KX_{from}$ and $V = W_VX_{from}$.\n",
        "\n",
        "Now, contrary to the dictionnary where queries and values are exact matchs, we don't have this here. Therefore, we will perform a softer match by computing the similarity matrix between $Q$ and $K$. Then for each $Q$, we want to output the values that have the higher similarity. We therefore output the weighted sum of the values, weighted by the softmax of the similarity (also called the attention matrix).\n",
        "\n",
        "Finally, the attention operation is given by the cross attention:\n",
        "\n",
        "$$\n",
        "A(Q,K,V) = \\text{SoftMax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "We divide the similarity by $\\sqrt{d_k}$ for stability reason to avoid the similarity to explode with big vectors which would lead to very sharp attention coeficients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP5bmjf4QqGt"
      },
      "source": [
        "#### Question 1.\n",
        "Implement the attention operation, use  `torch.einsum` to easily compute the similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L7uzzg6nQqGt"
      },
      "outputs": [],
      "source": [
        "# Xfrom is sequence from encoder output, which has to go through 2 layers (independently), to produce K and V which are used for cross attention in decoder\n",
        "# Xto is decoder input sequence, projected through a linear layer to produce Q\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, x_to_dim, x_from_dim, hidden_dim,):\n",
        "        # To complete\n",
        "        super(Attention, self).__init__()\n",
        "        self.x_to_proj = nn.Linear(x_to_dim, hidden_dim)\n",
        "        self.x_from_proj_key = nn.Linear(x_from_dim, hidden_dim)\n",
        "        self.x_from_proj_val = nn.Linear(x_from_dim, hidden_dim)\n",
        "        self.scale = hidden_dim ** 0.5\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x_to, x_from):\n",
        "        # x_to = [batch size, x_to_len, x_to_dim]\n",
        "        # x_from = [batch size, x_from_len, x_from_dim]\n",
        "\n",
        "        # To complete\n",
        "        Q = self.x_to_proj(x_to)  # [batch size, x_to_len, hidden_dim]\n",
        "        K = self.x_from_proj_key(x_from)  # [batch size, x_from_len, hidden_dim]\n",
        "        V = self.x_from_proj_val(x_from)  # [batch size, x_from_len, hidden_dim]\n",
        "\n",
        "        # similarity matrix using einsum\n",
        "        similarity = torch.einsum('bih,bjh->bij', Q, K)\n",
        "        similarity = similarity / self.scale\n",
        "\n",
        "        # attention weights\n",
        "        attention = F.softmax(similarity, dim=-1)\n",
        "\n",
        "        #apply attention weights to V\n",
        "        x_to = torch.einsum('bij,bjh->bih', attention, V)\n",
        "\n",
        "        return x_to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3kCoe-CQqGt"
      },
      "source": [
        "### Multi-head attention\n",
        "\n",
        "We improve the above attention implementation by introducing multi-head attention. The idea here is that we compute the attention on subspaces of the $Q,K,V$ triplets.\n",
        "We split each vector in $n$ subsets and compute the attention for each subset. At the end, we concatenate every attention output and project it with an output projection.\n",
        "\n",
        "#### Question 2.\n",
        "Implement Multihead attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WulZtTrkQqGt"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, x_to_dim, x_from_dim, hidden_dim, n_heads):\n",
        "        # To complete\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert hidden_dim % n_heads == 0, \"hidden_dim must be divisible by n_heads\"\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hidden_dim // n_heads\n",
        "        self.scale = self.head_dim ** 0.5\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.linear_q = nn.Linear(x_to_dim, hidden_dim)\n",
        "        self.linear_k = nn.Linear(x_from_dim, hidden_dim)\n",
        "        self.linear_v = nn.Linear(x_from_dim, hidden_dim)\n",
        "        self.output_linear = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x_to, x_from):\n",
        "        # x_to = [batch size, x_to_len, x_to_dim]\n",
        "        # x_from = [batch size, x_from_len, x_from_dim]\n",
        "\n",
        "        # To complete\n",
        "\n",
        "        #batch size is the number of samples processed in parallel,\n",
        "        #x to len is the length of each sequence (number of tokens), and\n",
        "        #x to dim is the dimensionality of each token representation\n",
        "\n",
        "        batch_size, x_to_len, _ = x_to.shape\n",
        "        _, x_from_len, _ = x_from.shape\n",
        "\n",
        "        Q = self.linear_q(x_to)  # [batch_size, x_to_len, hidden_dim]\n",
        "        K = self.linear_k(x_from)  # [batch_size, x_from_len, hidden_dim]\n",
        "        V = self.linear_v(x_from)  # [batch_size, x_from_len, hidden_dim]\n",
        "\n",
        "        #####\n",
        "        # reshape Q K V for multi-head attention\n",
        "        Q = Q.view(batch_size, x_to_len, self.n_heads, self.head_dim).transpose(1, 2)  # [batch_size, n_heads, x_to_len, head_dim]\n",
        "        K = K.view(batch_size, x_from_len, self.n_heads, self.head_dim).transpose(1, 2)  # [batch_size, n_heads, x_from_len, head_dim]\n",
        "        V = V.view(batch_size, x_from_len, self.n_heads, self.head_dim).transpose(1, 2)  # [batch_size, n_heads, x_from_len, head_dim]\n",
        "\n",
        "        attention_scores = torch.einsum('bnqd,bnkd->bnqk', Q, K) / self.scale\n",
        "\n",
        "        # softmax to get attention weights\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # attention weights to values using einsum\n",
        "        out = torch.einsum('bnqk,bnvd->bnqd', attention_weights, V)\n",
        "\n",
        "        #concat heads and apply output projection\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size,x_to_len,self.hidden_dim)\n",
        "        output = self.output_linear(out)\n",
        "        ######\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvOWL_zrQqGu"
      },
      "source": [
        "MultiheadAttention is the attention that is used in transformers in pratice. It is used in 2 flavors:\n",
        "- Self Attention: When $X_{to}$ attends itself ($X_{to}=X_{from}$)\n",
        "- Cross Attention. $X_{to}\\neq X_{from}$\n",
        "\n",
        "\n",
        "#### Question 3.\n",
        "Implement MultiHead Self Attention and MultiHeadCrossAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pjwscVQ-QqGu"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(MultiHeadAttention):\n",
        "    def __init__(self, x_dim, hidden_dim, n_heads):\n",
        "        # To complete\n",
        "        super(MultiHeadSelfAttention, self).__init__(x_to_dim=x_dim, x_from_dim=x_dim,\n",
        "                                                     hidden_dim=hidden_dim, n_heads=n_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, x_len, x_dim]\n",
        "        # To complete\n",
        "        return super(MultiHeadSelfAttention, self).forward(x_to=x, x_from=x)\n",
        "\n",
        "class MultiHeadCrossAttention(MultiHeadAttention):\n",
        "    def __init__(self, x_to_dim, x_from_dim, hidden_dim, n_heads):\n",
        "        # To complete\n",
        "        super(MultiHeadCrossAttention, self).__init__(x_to_dim=x_to_dim, x_from_dim=x_from_dim,\n",
        "                                                     hidden_dim=hidden_dim, n_heads=n_heads)\n",
        "\n",
        "    def forward(self, x_to, x_from):\n",
        "        # x_to = [batch size, x_to_len, x_to_dim]\n",
        "        # x_from = [batch size, x_from_len, x_from_dim]\n",
        "\n",
        "        # To complete\n",
        "        return super(MultiHeadCrossAttention, self).forward(x_to=x_to, x_from=x_from)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVmrKOpKQqGu"
      },
      "source": [
        "### LayerNorm\n",
        "Another key component of the transformer is the LayerNorm. As we have previously seen, normalizing the output of a deep learning layer helps a lot with convergence and stability.\n",
        "Until Transformers, the most used normalization is BatchNorm. We normalize the data among the batch dimension. However, this has a few problems.\n",
        "- The normalization depend on the other samples in the batch\n",
        "- When using multiple GPUs, BatchNorm needs to synchronize the batch statistic across GPUs, which locks the forward process and slow down training.\n",
        "\n",
        "The last element is the most important one. Transformers, aims to be a easy to parralilize architecture and can't afford to use batchnorm.\n",
        "\n",
        "Instead, Transformers uses Layer Norm. LayerNorm is sample dependent, which removes the synchronization issue. We normalize over the channel dimension instead of the batch dimension.\n",
        "\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png\">\n",
        "\n",
        "To account for the loss of capacity, we map the output by a linear transformation with a learned bias and scale.\n",
        "\n",
        "#### Question 4.\n",
        "Implement the LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VSNVM-koQqGu"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    # To complete\n",
        "    def __init__(self, parameters_shape, dim_to_norm=-1, eps=1e-5):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.dim_to_norm = dim_to_norm\n",
        "\n",
        "        #learnable parameters\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=self.dim_to_norm, keepdim=True)\n",
        "        var = x.var(dim=self.dim_to_norm, keepdim=True, unbiased=False)\n",
        "\n",
        "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
        "\n",
        "        return self.gamma * x_normalized + self.beta\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvXarGQKQqGu"
      },
      "source": [
        "### Feed Feedward Network\n",
        "\n",
        "Finally, the last block is a feed-forward network with one hidden layer. This layer has usually a size of $2 * input\\_dim$. This is followed by a dropout layer and an activation function. Here, we will use leaky relu, with a leak parameter of 0.1.\n",
        "\n",
        "#### Question 5.\n",
        "Implement the FFN layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RjVZu9ujQqGu"
      },
      "outputs": [],
      "source": [
        "class FFN(nn.Sequential):\n",
        "    def __init__(self, hidden_dim, dropout_rate=0.1, expansion_factor=2):\n",
        "        # To complete\n",
        "        super(FFN, self).__init__(\n",
        "            nn.Linear(hidden_dim, hidden_dim * expansion_factor),\n",
        "            nn.LeakyReLU(negative_slope=0.1),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim * expansion_factor, hidden_dim)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbbLC6VZQqGu"
      },
      "source": [
        "### The Transformer block\n",
        "\n",
        "The last thing that we are missing are the skip connection. Like in ResNet, the transformer architecture implements the skip-connection. This allow for a better gradient flow avoiding vanishing gradient.\n",
        "There is a skip connection after the attention and the feed forward network\n",
        "\n",
        "#### Question 6.\n",
        "Given at the transformer figure at the top, implement the Transformer Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Zci4MBmWQqGu"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self ,data_dim, hidden_dim, n_heads, dropout_rate=0.1):\n",
        "        # To complete\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "\n",
        "\n",
        "        self.multi_head_attention = MultiHeadSelfAttention(data_dim, hidden_dim, n_heads)\n",
        "        self.layer_norm1 = LayerNorm(hidden_dim)\n",
        "        self.ffn = FFN(hidden_dim, dropout_rate=dropout_rate)\n",
        "        self.layer_norm2 = LayerNorm(hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, x_len, hidden dim]\n",
        "        # To complete\n",
        "        multi_head_attention_output = self.multi_head_attention(x)\n",
        "        x = self.layer_norm1(x + multi_head_attention_output)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.layer_norm2(x + ffn_output)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmmNJlZFQqGu"
      },
      "source": [
        "### Positional embedding\n",
        "The transformers architecture is permutation independent. That means that for every token, we can swap 2 tokens and have the exact same result. However, the position of the token can be a very important information to consider. Imagine in an image. If a pixel is nearby another pixel, we want the transformer to be able to capture such information. Which is not the case for now.\n",
        "That's why we introduce positional encodings. For each token, add the positional encoding to the original token:\n",
        "\n",
        "$$\n",
        "X_i = X_i + PE(i)\n",
        "$$\n",
        "\n",
        "with X_i the token at the i dimension.\n",
        "\n",
        "The most used positional encodings are sinusoidal encodings. They are defined as follow:\n",
        "\n",
        "$$\n",
        "PE(i, 2j) = sin(i / 10000^{\\frac{2j}{d}}) \\\\\n",
        "PE(i, 2j + 1) = cos(i / 10000^{\\frac{2j}{d}})\n",
        "$$\n",
        "\n",
        "\n",
        "Where $d$ the dimension of the tokens, $i$, the i-th token in the sequence and $2j$ (resp $2j + 1$), the index of the dimension of the vector.\n",
        "The idea here is that we add a sinusoidal that encode the position in a multidimensional array.\n",
        "\n",
        "Another common positional encodings is the learned positional encoding. Simply, we let the network learn a set of tensor $PE$ that match the sequence length and dimension of the tokens.\n",
        "\n",
        "#### Question 7.\n",
        "\n",
        "Implement both Sinusoidal and Learned positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o6rK8aYeQqGu"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(SinusoidalPositionalEncoding, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, seq len, hidden dim]\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        # Tensor for positional encodings, no batch dimension needed\n",
        "        PE_mat = torch.zeros(seq_len, self.hidden_dim)\n",
        "\n",
        "        position = torch.arange(seq_len).unsqueeze(1)  # [seq_len, 1]\n",
        "\n",
        "        # div_term calculation adjusted for the entire hidden_dim\n",
        "        #log for numerical stability and exp to adhere back to formula\n",
        "        div_term = torch.exp(torch.arange(0, self.hidden_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / (self.hidden_dim // 2)))\n",
        "\n",
        "        # sin to even indices\n",
        "        PE_mat[:, 0::2] = torch.sin(position * div_term)  # [seq_len, hidden_dim/2]\n",
        "\n",
        "        # cos to odd indices\n",
        "        PE_mat[:, 1::2] = torch.cos(position * div_term)  # [seq_len, hidden_dim/2]\n",
        "\n",
        "        # expand to match batch size\n",
        "        PE_mat = PE_mat.unsqueeze(0).expand(batch_size, -1, -1)  # [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        return x + PE_mat\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_dim, max_len):\n",
        "        # To complete\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.max_len = max_len\n",
        "        #learnable param\n",
        "        self.PE = nn.Parameter(torch.randn(1, max_len, hidden_dim))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, seq len, hidden dim]\n",
        "        # To complete\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        PE_mat = self.PE[:, :seq_len, :]\n",
        "        # match batch size\n",
        "        PE_mat = PE_mat.expand(batch_size, -1, -1)\n",
        "        return x + PE_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJdpld00QqGu"
      },
      "source": [
        "### The transformer encoder\n",
        "Now you have everything you need to implement the transformer . You add positional encoding to the tokens and then stack N transformer encoder layers\n",
        "\n",
        "#### Question 8.\n",
        "Implement the transformer encoder with n_layers and the ability to choose both positional embeddings.\n",
        "\n",
        "Tip: Look into `ModuleList`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "scxhANBFQqGv"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, data_dim,  hidden_dim, n_heads, n_layers, dropout_rate=0.1, positional_encoding=\"sinusoidal\", max_len=1000):\n",
        "        # To complete\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.positional_encoding = SinusoidalPositionalEncoding(hidden_dim) if positional_encoding == \"sinusoidal\" else LearnedPositionalEncoding(hidden_dim, max_len)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderBlock(data_dim, hidden_dim, n_heads, dropout_rate) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, seq len, hidden dim]\n",
        "        # To complete\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz_VCLkvQqGv"
      },
      "source": [
        "# Part 2. Classical Architectures: ViT & CCT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zv_Tz2kQqGv"
      },
      "source": [
        "## The Vision Transformer\n",
        "The above architecture was introduced in 2017 to process sequences of text tokens. However, it could be useful to be able to leverage this architecture for computer vision. On the contrary of convolutional neural network, the transformer has the advantage to introduce less inductive bias.\n",
        "\n",
        "This could be interesting to leverage to improve vision systems. If we learn the biases from the data, we can hope to have better performances. We however need compute and a lot of data to do this.\n",
        "\n",
        "To apply the transformer to images, one key question remains to be answered: How do we transform an image to tokens? The approach introduce in Vision Transformers is to cut the image into patches that are then transformed into a token trhought a linear projection.\n",
        "\n",
        "We also add an extra token, known as the classification token, that will be the token which will be use to predict upon. After going through the N transformer layers, this is the token that goes throught a multi layer perceptron.\n",
        "\n",
        "\n",
        "<img src= \"https://1.bp.blogspot.com/-_mnVfmzvJWc/X8gMzhZ7SkI/AAAAAAAAG24/8gW2AHEoqUQrBwOqjhYB37A7OOjNyKuNgCLcBGAsYHQ/s1600/image1.gif\" width=\"512\">\n",
        "\n",
        "\n",
        "#### Question 9\n",
        "\n",
        "Implement the vision transformer\n",
        "\n",
        "Hint: Use Conv2D with the right kernel size and stride to do the linear projection of non-overlapping patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-kBLyLg5QqGv"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, patch_size, hidden_dim, n_heads, n_layers, n_classes, dropout_rate=0.1, positional_encoding=\"sinusoidal\", max_len=1000, num_classes=10):\n",
        "        super(ViT, self).__init__()\n",
        "        # To complete\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.patch_embedding = nn.Conv2d(in_channels=3, out_channels=hidden_dim,\n",
        "                                         kernel_size=patch_size, stride=patch_size)\n",
        "        self.transformer_encoder = TransformerEncoder(hidden_dim, hidden_dim, n_heads, n_layers, dropout_rate, positional_encoding, max_len)\n",
        "        self.classification_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "\n",
        "        self.mlp_head = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, 3, image height, image width]\n",
        "        batch_size, _, height, width = x.shape\n",
        "        # To complete\n",
        "\n",
        "        # seperate each image into patches\n",
        "        x = self.patch_embedding(x)  # [batch_size, hidden_dim, height/patch_size, width/patch_size]\n",
        "        x = x.flatten(2).transpose(1, 2)  #[batch_size, num_patches, hidden_dim]\n",
        "\n",
        "        # add classification token to the sequence\n",
        "        cls_tokens = self.classification_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # pass through transformer encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # classification token goes through the final MLP\n",
        "        cls_token = x[:, 0, :]\n",
        "        x = self.mlp_head(cls_token)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P327yRW6QqGv"
      },
      "source": [
        "#### Question 10.\n",
        "Train a ViT on CIFAR10 for 100 epochs (for compute reason you can use only 20 epochs) and log both train and test loss and accuracy.\n",
        "We provide a data augmentation strategy called auto augment to avoid overfitting on the training data.\n",
        "Hparameters are to be choosen to your discretion.\n",
        "\n",
        "\n",
        "Tips for Hparams:\n",
        "- Don't use a transformer hidden dim too big (<256)\n",
        "- Use a small patch size\n",
        "- Use AdamW with some weight decay to avoid overfitting\n",
        "- Use between 2 and 6 transformer layers.\n",
        "- Use between 2 and 4 transformer heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCS02hPLQqGv"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_set = CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose([\n",
        "    transforms.autoaugment.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "]))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "test_set = CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "]))\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrGbrYbJQqGv"
      },
      "outputs": [],
      "source": [
        "# To complete: train the model, (don't forget to test it)\n",
        "# hyperparameters\n",
        "patch_size = 4\n",
        "hidden_dim = 128\n",
        "n_heads = 4\n",
        "n_layers = 4\n",
        "dropout_rate = 0.1\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.01  # for AdamW\n",
        "\n",
        "model = ViT(patch_size=patch_size, hidden_dim=hidden_dim, n_heads=n_heads, n_layers=n_layers, n_classes=n_classes, dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # training metrics\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = 100. * correct / total\n",
        "\n",
        "    # testing / validation\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # testing metrics\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = 100. * correct / total\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}]')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############ TODO######\n",
        "\n",
        "# ACTUALLY RUN THIS, COULD NOT FOR NOW BECAUSE CANNOT USE GPU NOT ENOUGH CREDITS :(\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hrL17pjQqGv"
      },
      "outputs": [],
      "source": [
        "# Plot the training and test loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYoOwt6rQqGv"
      },
      "source": [
        "## Compact Convolutional Transformer\n",
        "The previous network requires a lot of compute and data to be trained. As we mentionned before, the transformer removes the inductive bias of convnets which requires more data to be trained.\n",
        "\n",
        "To bypass this, let's try out another architecture. We will try an hybrid architecture that preserves the inductive biases of convolution but manages to use the transformer to add global learning.\n",
        "\n",
        "The first change is the tokenizer. We replace it with a ConvNet. Each convnet layer has a convolution, ReLU and maxpooling.\n",
        "The second change is to actually remove the classfication token and classify on top of a pooling of all tokens. The pooling is done with an attention like mechanism:\n",
        "- For each sample, we predict a scalar, that we compute the softmax over all the sample tokens.\n",
        "- We then do an weighted average pool by this softmax values over the tokens. The weight is given by the previous step\n",
        "\n",
        "More details see: https://arxiv.org/abs/2104.05704\n",
        "\n",
        "<img src= https://miro.medium.com/v2/resize:fit:720/format:webp/1*8diH01Fl7MhHRemLy9hUHw.png width=512>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxUE6DQvQqGv"
      },
      "source": [
        "#### Question 11.\n",
        "Implement the Convolutional based tokenizer and the SeqPool operationm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H194VS99QqGv"
      },
      "outputs": [],
      "source": [
        "class ConvPatchEmbedding(nn.Module):\n",
        "    def __init__(self, n_layers, kernel_size, hidden_dim):\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, 3, image height, image width]\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "class SeqPool(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, seq len, hidden dim]\n",
        "        # To complete\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA88_aPOQqGv"
      },
      "source": [
        "#### Question 12.\n",
        "\n",
        "Implement the Compact Convolutional Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaLaCOvBQqGv"
      },
      "outputs": [],
      "source": [
        "class CCT(nn.Module):\n",
        "    def __init__(self, n_conv_layers, kernel_size,  n_transformer_layers, hidden_dim, n_heads, n_classes, dropout_rate=0.1):\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, 3, image height, image width]\n",
        "        # To complete\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf8jbhb6QqGv"
      },
      "source": [
        "#### Question 13.\n",
        "Train the CCT on CIFAR-10 for 100 epochs (for compute reason you can use only 20 epochs) and log both train and test loss and accuracy. You should obtain at least 75+% test accuracy, and observe a improvement compared to the previous ViT (Possible to get 90%+).\n",
        "We provide a data augmentation strategy called auto augment to avoid overfitting on the training data.\n",
        "Hparameters are to be choosen to your discretion.\n",
        "\n",
        "Tips for Hparams:\n",
        "- Don't use too big of a transformer hidden dim (<256)\n",
        "- For the convnet, aim to have between 32 and 128 output tokens.\n",
        "- Use AdamW with some weight decay to avoid overfitting\n",
        "- Use between 2 and 6 transformer layers.\n",
        "- Use between 2 and 4 transformer heads\n",
        "\n",
        "Training takes around 30min (depending of hparams)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWR8FggcQqGw"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_set = CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose([\n",
        "    transforms.autoaugment.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "]))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "test_set = CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "]))\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8whO5sbQqGw"
      },
      "outputs": [],
      "source": [
        "# To complete: train the model, (don't forget to test it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGGqTW-IQqGw"
      },
      "outputs": [],
      "source": [
        "# Plot the training and test loss"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}